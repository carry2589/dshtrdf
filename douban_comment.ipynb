{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import jieba\n",
    "import numpy\n",
    "import codecs\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 设置递归深度\n",
    "sys.setrecursionlimit(2000)\n",
    "    \n",
    "    \n",
    "# 解析电影页\n",
    "def parse_subject(url):\n",
    "    r = s.get(url, headers=headers, timeout=5)\n",
    "    html = r.text\n",
    "    soup = bs(html, 'html.parser')\n",
    "    title = soup.find('span', attrs={'property': 'v:itemreviewed'}).text\n",
    "    year = soup.find('span', class_='year').text\n",
    "    pic = soup.find('a', class_='nbgnbg').find('img').get('src')\n",
    "    info = soup.find('div', id='info').text\n",
    "    summary = soup.find('span', attrs={'property': 'v:summary'}).text\n",
    "    return title, year, pic, info, summary\n",
    "\n",
    "# 解析短评\n",
    "def parse_comment(url, headers):\n",
    "    global comments, page\n",
    "    r = s.get(url, headers=headers, timeout=5)\n",
    "    html = r.text\n",
    "    soup = bs(html, 'html.parser')\n",
    "    comment_divs = soup.find_all('div', class_='comment')\n",
    "    if comment_divs is not None:\n",
    "        comments += ''.join([item.find('p').text.strip() for item in comment_divs if item.find('p') != None])\n",
    "    next_page = soup.find('a', class_='next')\n",
    "    time.sleep(1)  #适当休息以降低请求频率\n",
    "    if next_page is not None and page < 30:\n",
    "        page += 1\n",
    "        return parse_comment(urljoin(next_page.get('href'), url), headers)\n",
    "    return comments\n",
    "\n",
    "def generate_cloud():\n",
    "    global comments\n",
    "    pattern = re.compile(r'[\\u4e00-\\u9fa5]+')\n",
    "    filterdata = re.findall(pattern, comments)\n",
    "    new_comments = ''.join(filterdata)\n",
    "\n",
    "    #使用结巴分词进行中文分词\n",
    "    segment = jieba.lcut(new_comments)\n",
    "    words_df = pd.DataFrame({'segment':segment})\n",
    "\n",
    "    #去掉停用词\n",
    "    stopwords = pd.read_csv('stopwords.txt', index_col=False, quoting=3, sep='\\t', names=['stopword'], encoding='utf-8')#quoting=3全不引用\n",
    "    words_df = words_df[~words_df.segment.isin(stopwords.stopword)]\n",
    "\n",
    "    #统计词频\n",
    "    words_stat = words_df.groupby(by=['segment'])['segment'].agg({\"计数\":numpy.size})\n",
    "    words_stat = words_stat.reset_index().sort_values(by=[\"计数\"], ascending=False)\n",
    "\n",
    "    #用词云进行显示\n",
    "    wordcloud = WordCloud(font_path='simhei.ttf', background_color='white', max_font_size=80)\n",
    "    word_frequence = {x[0]:x[1] for x in words_stat.head(1000).values}\n",
    "\n",
    "    wordcloud = wordcloud.fit_words(word_frequence)\n",
    "    plt.imshow(wordcloud)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    s = requests.Session()\n",
    "    cookie = input('请把登录后获取到的cookie粘贴在这里:\\n')\n",
    "    headers = {\n",
    "    'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Encoding':'gzip, deflate, br',\n",
    "    'Accept-Language':'zh,en;q=0.9,en-US;q=0.8,zh-CN;q=0.7',\n",
    "    'Cache-Control':'no-cache',\n",
    "    'Cookie':cookie,\n",
    "    'Connection':'keep-alive',\n",
    "    'Host':'movie.douban.com',\n",
    "    'Pragma':'no-cache',\n",
    "    'Referer':'https://movie.douban.com/',\n",
    "    'Upgrade-Insecure-Requests':'1',\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36' \n",
    "    }\n",
    "    comments = ''\n",
    "    page = 1\n",
    "    url = input('请输入豆瓣电影地址:\\n')\n",
    "    title, year, pic, info, summary = parse_subject(url)\n",
    "    print(title, year, '\\n')\n",
    "    print(info, '\\n\\n')\n",
    "    print(summary, '\\n\\n')\n",
    "    print('\\n================================\\n\\n')\n",
    "    comment_url = urljoin(url, 'comments?start=0&limit=20&sort=new_score&status=P&percent_type=')\n",
    "    parse_comment(comment_url, headers)\n",
    "    generate_cloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
